{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Define the emotions to be considered\n",
    "valid_emotions = [\"Aversion\", \"Anger\", \"Peace\", \"Happiness\", \"Sadness\", \"Fear\", \"Surprise\"]\n",
    "\n",
    "def modify_image_folder_path(image_folder, valid_emotion):\n",
    "    # Replace \"ThirdPart\" with the valid emotion\n",
    "    modified_image_folder = image_folder.replace(\"ThirdPart\", valid_emotion)\n",
    "    return modified_image_folder\n",
    "\n",
    "def process_and_crop_images(csv_data):\n",
    "    for index, row in csv_data.iterrows():\n",
    "        image_folder = row['Video Path']\n",
    "        emotion_values = row['Emotion Value'].split(',')\n",
    "        start_frame = row['Start Frame']\n",
    "        end_frame = row['End Frame']\n",
    "        npy_path = row['Npy Path']\n",
    "        \n",
    "        # Process each emotion value separately\n",
    "        for emotion in emotion_values:\n",
    "            valid_emotion = emotion.strip()\n",
    "            if valid_emotion in valid_emotions:\n",
    "                # Modify the image folder path based on the valid emotion\n",
    "                modified_image_folder = modify_image_folder_path(image_folder, valid_emotion)\n",
    "                \n",
    "                # Check if the modified image folder exists\n",
    "                if not os.path.exists(modified_image_folder):\n",
    "                    print(f\"Error: Modified image folder '{modified_image_folder}' does not exist for emotion '{valid_emotion}'.\")\n",
    "                    continue\n",
    "                \n",
    "                # Debugging: Print the modified image folder path\n",
    "                print(f\"Processing images for emotion '{valid_emotion}' in folder '{modified_image_folder}'\")\n",
    "\n",
    "                # Load the .npy file for this specific folder\n",
    "                if not os.path.exists(npy_path):\n",
    "                    print(f\"Error: NPY file '{npy_path}' does not exist for emotion '{valid_emotion}'.\")\n",
    "                    continue\n",
    "                \n",
    "                npy_data = np.load(npy_path)\n",
    "                \n",
    "                # Calculate the number of frames to process\n",
    "                frame_diff = end_frame - start_frame\n",
    "                \n",
    "                # Filter the npy data for the given frames\n",
    "                frame_data = npy_data[start_frame:end_frame+1]\n",
    "                \n",
    "                for frame in frame_data:\n",
    "                    frame_number = int(frame[0])\n",
    "                    person_id = int(frame[1])\n",
    "                    joint_coordinates = frame[2:].reshape((18, 3))\n",
    "                    \n",
    "                    # Load the corresponding image\n",
    "                    image_path = os.path.join(modified_image_folder, f\"{frame_number:06d}.jpg\")\n",
    "                    \n",
    "                    # Debugging: Print the image path being checked\n",
    "                    print(f\"Looking for image at path '{image_path}'\")\n",
    "                    \n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Error: Image '{image_path}' does not exist for frame '{frame_number}' and emotion '{valid_emotion}'.\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        image = Image.open(image_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: Failed to open image '{image_path}' for frame '{frame_number}' and emotion '{valid_emotion}'. Exception: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    cropped_images = crop_image(image, joint_coordinates)\n",
    "                    \n",
    "                    if not cropped_images:\n",
    "                        print(f\"Error: No cropped images generated for frame '{frame_number}' and emotion '{valid_emotion}'.\")\n",
    "                    \n",
    "                    # Save the cropped images\n",
    "                    for i, cropped_image in enumerate(cropped_images):\n",
    "                        cropped_image_path = os.path.join(modified_image_folder, f\"{frame_number:06d}_person_{person_id}_joint_{i}.jpg\")\n",
    "                        try:\n",
    "                            cropped_image.save(cropped_image_path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error: Failed to save cropped image '{cropped_image_path}' for frame '{frame_number}', person '{person_id}', joint '{i}' and emotion '{valid_emotion}'. Exception: {e}\")\n",
    "            else:\n",
    "                # Print an error message if no valid emotion is found\n",
    "                print(f\"Error: '{valid_emotion}' is not a valid emotion for row {index}. Skipping this emotion.\")\n",
    "\n",
    "def crop_image(image, joint_coordinates):\n",
    "    cropped_images = []\n",
    "    for i, joint in enumerate(joint_coordinates):\n",
    "        x, y, confidence = joint\n",
    "        if confidence > 0.5:  # Assuming a threshold for confidence\n",
    "            left = max(0, x - 50)\n",
    "            top = max(0, y - 50)\n",
    "            right = min(image.width, x + 50)\n",
    "            bottom = min(image.height, y + 50)\n",
    "            cropped_image = image.crop((left, top, right, bottom))\n",
    "            cropped_images.append(cropped_image)\n",
    "    return cropped_images\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"E:\\dummy.csv\"\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Process and crop images\n",
    "process_and_crop_images(csv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the .npy file\n",
    "data = np.load(r\"D:\\BoLD\\BOLD_public\\joints\\003\\IzvOYVMltkI.mp4\\0114.npy\")\n",
    "\n",
    "# Convert the numpy array to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a .csv file\n",
    "df.to_csv('data3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Define the emotions to be considered\n",
    "valid_emotions = [\"Aversion\", \"Anger\", \"Peace\", \"Happiness\", \"Sadness\", \"Fear\", \"Surprise\"]\n",
    "\n",
    "def modify_image_folder_path(image_folder, valid_emotion):\n",
    "    # Replace \"ThirdPart\" with the valid emotion\n",
    "    modified_image_folder = image_folder.replace(\"ThirdPart\", valid_emotion)\n",
    "    return modified_image_folder\n",
    "\n",
    "def process_and_crop_images(csv_data):\n",
    "    for index, row in csv_data.iterrows():\n",
    "        image_folder = row['Video Path']\n",
    "        emotion_values = row['Emotion Value'].split(',')\n",
    "        start_frame = row['Start Frame']\n",
    "        end_frame = row['End Frame']\n",
    "        npy_path = row['Npy Path']\n",
    "        \n",
    "        # Process each emotion value separately\n",
    "        for emotion in emotion_values:\n",
    "            valid_emotion = emotion.strip()\n",
    "            if valid_emotion in valid_emotions:\n",
    "                # Modify the image folder path based on the valid emotion\n",
    "                modified_image_folder = modify_image_folder_path(image_folder, valid_emotion)\n",
    "                \n",
    "                # Check if the modified image folder exists\n",
    "                if not os.path.exists(modified_image_folder):\n",
    "                    print(f\"Error: Modified image folder '{modified_image_folder}' does not exist for emotion '{valid_emotion}'.\")\n",
    "                    continue\n",
    "                \n",
    "                # Debugging: Print the modified image folder path\n",
    "                print(f\"Processing images for emotion '{valid_emotion}' in folder '{modified_image_folder}'\")\n",
    "\n",
    "                # Load the .npy file for this specific folder\n",
    "                if not os.path.exists(npy_path):\n",
    "                    print(f\"Error: NPY file '{npy_path}' does not exist for emotion '{valid_emotion}'.\")\n",
    "                    continue\n",
    "                \n",
    "                npy_data = np.load(npy_path)\n",
    "                \n",
    "                # Calculate the number of frames to process\n",
    "                frame_count = end_frame - start_frame + 1  # Include both start and end frames\n",
    "                \n",
    "                # Ensure we don't exceed the available data in npy file\n",
    "                if frame_count > len(npy_data):\n",
    "                    print(f\"Error: The number of frames to process ({frame_count}) exceeds the available data in the npy file ({len(npy_data)}).\")\n",
    "                    continue\n",
    "                \n",
    "                # Process the frames within the specified range\n",
    "                for i in range(frame_count):\n",
    "                    frame_number = start_frame + i\n",
    "                    frame = npy_data[i]  # Get the i-th row from the npy file\n",
    "                    person_id = int(frame[1])\n",
    "                    joint_coordinates = frame[2:].reshape((18, 3))\n",
    "                    \n",
    "                    # Load the corresponding image\n",
    "                    image_path = os.path.join(modified_image_folder, f\"frame_{frame_number}.jpg\")\n",
    "                    \n",
    "                    # Debugging: Print the image path being checked\n",
    "                    print(f\"Looking for image at path '{image_path}'\")\n",
    "                    \n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Error: Image '{image_path}' does not exist for frame '{frame_number}' and emotion '{valid_emotion}'.\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        image = Image.open(image_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: Failed to open image '{image_path}' for frame '{frame_number}' and emotion '{valid_emotion}'. Exception: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    cropped_image = crop_image(image, joint_coordinates)\n",
    "                    \n",
    "                    if cropped_image is None:\n",
    "                        print(f\"Error: No cropped images generated for frame '{frame_number}' and emotion '{valid_emotion}'.\")\n",
    "                    \n",
    "                    # Save the cropped image\n",
    "                    cropped_image_path = image_path  # Save the cropped image to the original path\n",
    "                    try:\n",
    "                        cropped_image.save(cropped_image_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: Failed to save cropped image '{cropped_image_path}' for frame '{frame_number}', person '{person_id}' and emotion '{valid_emotion}'. Exception: {e}\")\n",
    "            else:\n",
    "                # Print an error message if no valid emotion is found\n",
    "                print(f\"Error: '{valid_emotion}' is not a valid emotion for row {index}. Skipping this emotion.\")\n",
    "\n",
    "def crop_image(image, joint_coordinates):\n",
    "    # Determine the bounding box for all joints\n",
    "    min_x = min(joint_coordinates[:, 0])\n",
    "    min_y = min(joint_coordinates[:, 1])\n",
    "    max_x = max(joint_coordinates[:, 0])\n",
    "    max_y = max(joint_coordinates[:, 1])\n",
    "\n",
    "    # Ensure coordinates are within image bounds\n",
    "    top_left_x = max(int(min_x), 0)\n",
    "    top_left_y = max(int(min_y), 0)\n",
    "    bottom_right_x = min(int(max_x), image.width)\n",
    "    bottom_right_y = min(int(max_y), image.height)\n",
    "\n",
    "    # Crop image\n",
    "    cropped_image = image.crop((top_left_x, top_left_y, bottom_right_x, bottom_right_y))\n",
    "    return cropped_image\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"E:\\annotation_train_2.0.csv\"\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Process and crop images\n",
    "process_and_crop_images(csv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(r\"D:\\My Files\\Downloads - V\\yolov3.weights\", r\"D:\\My Files\\Downloads - V\\yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "# Adjust for different return types of getUnconnectedOutLayers\n",
    "try:\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "except IndexError:\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(r\"D:\\My Files\\Downloads - V\\coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "def detect_and_crop(image_path, output_path):\n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    # Detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Information to show on the screen\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # For each detection from each output layer, get the confidence, class id, bounding box params\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and classes[class_id] == \"person\":  # Filter for 'person'\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-max suppression to eliminate redundant overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Loop through the detected boxes\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            # Crop the detected human\n",
    "            cropped_img = img[y:y+h, x:x+w]\n",
    "            # Save the cropped image\n",
    "            output_filename = os.path.join(output_path, os.path.basename(image_path))\n",
    "            cv2.imwrite(output_filename, cropped_img)\n",
    "\n",
    "# Example usage\n",
    "input_folder = r\"E:\\cropped_images\\video_110\"\n",
    "output_folder = r\"E:\\crop\"\n",
    "\n",
    "# Process all images in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "        input_image_path = os.path.join(input_folder, filename)\n",
    "        detect_and_crop(input_image_path, output_folder)\n",
    "\n",
    "\n",
    "# Crop images if it recognizes as human\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO Detection with Cropping based on Joints coordinates\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Define the emotions to be considered\n",
    "valid_emotions = [\"Aversion\", \"Anger\", \"Peace\", \"Happiness\", \"Sadness\", \"Fear\", \"Surprise\"]\n",
    "\n",
    "def modify_image_folder_path(image_folder, valid_emotion):\n",
    "    # Replace \"ThirdPart\" with the valid emotion\n",
    "    modified_image_folder = image_folder.replace(\"ThirdPart\", valid_emotion)\n",
    "    return modified_image_folder\n",
    "\n",
    "def process_and_crop_images(csv_data):\n",
    "    for index, row in csv_data.iterrows():\n",
    "        image_folder = row['Video Path']\n",
    "        emotion_values = row['Emotion Value'].split(',')\n",
    "        start_frame = row['Start Frame']\n",
    "        end_frame = row['End Frame']\n",
    "        npy_path = row['Npy Path']\n",
    "        \n",
    "        # Process each emotion value separately\n",
    "        for emotion in emotion_values:\n",
    "            valid_emotion = emotion.strip()\n",
    "            if valid_emotion in valid_emotions:\n",
    "                # Modify the image folder path based on the valid emotion\n",
    "                modified_image_folder = modify_image_folder_path(image_folder, valid_emotion)\n",
    "                \n",
    "                # Check if the modified image folder exists\n",
    "                if not os.path.exists(modified_image_folder):\n",
    "                    print(f\"Error: Modified image folder '{modified_image_folder}' does not exist for emotion '{valid_emotion}'.\")\n",
    "                    continue\n",
    "                \n",
    "                # Debugging: Print the modified image folder path\n",
    "                print(f\"Processing images for emotion '{valid_emotion}' in folder '{modified_image_folder}'\")\n",
    "\n",
    "                # Load the .npy file for this specific folder\n",
    "                if not os.path.exists(npy_path):\n",
    "                    print(f\"Error: NPY file '{npy_path}' does not exist for emotion '{valid_emotion}'.\")\n",
    "                    continue\n",
    "                \n",
    "                npy_data = np.load(npy_path)\n",
    "                \n",
    "                # Calculate the number of frames to process\n",
    "                frame_count = end_frame - start_frame + 1  # Include both start and end frames\n",
    "                \n",
    "                # Ensure we don't exceed the available data in npy file\n",
    "                if frame_count > len(npy_data):\n",
    "                    print(f\"Error: The number of frames to process ({frame_count}) exceeds the available data in the npy file ({len(npy_data)}).\")\n",
    "                    continue\n",
    "                \n",
    "                # Process the frames within the specified range\n",
    "                for i in range(frame_count):\n",
    "                    frame_number = start_frame + i\n",
    "                    frame = npy_data[i]  # Get the i-th row from the npy file\n",
    "                    person_id = int(frame[1])\n",
    "                    joint_coordinates = frame[2:].reshape((18, 3))\n",
    "                    \n",
    "                    # Load the corresponding image\n",
    "                    image_path = os.path.join(modified_image_folder, f\"frame_{frame_number}.jpg\")\n",
    "                    \n",
    "                    # Debugging: Print the image path being checked\n",
    "                    print(f\"Looking for image at path '{image_path}'\")\n",
    "                    \n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Error: Image '{image_path}' does not exist for frame '{frame_number}' and emotion '{valid_emotion}'.\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        image = Image.open(image_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: Failed to open image '{image_path}' for frame '{frame_number}' and emotion '{valid_emotion}'. Exception: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Detect and crop the human closest to the joint coordinates\n",
    "                    cropped_image = detect_and_crop(image, joint_coordinates)\n",
    "                    \n",
    "                    if cropped_image is None:\n",
    "                        print(f\"Error: No cropped images generated for frame '{frame_number}' and emotion '{valid_emotion}'.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Save the cropped image\n",
    "                    cropped_image_path = image_path  # Save the cropped image to the original path\n",
    "                    try:\n",
    "                        cropped_image.save(cropped_image_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: Failed to save cropped image '{cropped_image_path}' for frame '{frame_number}', person '{person_id}' and emotion '{valid_emotion}'. Exception: {e}\")\n",
    "            else:\n",
    "                # Print an error message if no valid emotion is found\n",
    "                print(f\"Error: '{valid_emotion}' is not a valid emotion for row {index}. Skipping this emotion.\")\n",
    "\n",
    "def crop_image(image, joint_coordinates):\n",
    "    # Determine the bounding box for all joints\n",
    "    min_x = min(joint_coordinates[:, 0])\n",
    "    min_y = min(joint_coordinates[:, 1])\n",
    "    max_x = max(joint_coordinates[:, 0])\n",
    "    max_y = max(joint_coordinates[:, 1])\n",
    "\n",
    "    # Ensure coordinates are within image bounds\n",
    "    top_left_x = max(int(min_x), 0)\n",
    "    top_left_y = max(int(min_y), 0)\n",
    "    bottom_right_x = min(int(max_x), image.width)\n",
    "    bottom_right_y = min(int(max_y), image.height)\n",
    "\n",
    "    # Crop image\n",
    "    cropped_image = image.crop((top_left_x, top_left_y, bottom_right_x, bottom_right_y))\n",
    "    return cropped_image\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(r\"E:\\YOLO\\yolov3.weights\", r\"E:\\YOLO\\yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "# Adjust for different return types of getUnconnectedOutLayers\n",
    "try:\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "except IndexError:\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(r\"E:\\YOLO\\coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "def detect_and_crop(image, joint_coordinates):\n",
    "    # Convert PIL image to OpenCV format\n",
    "    img = np.array(image)\n",
    "    img = img[:, :, ::-1].copy()  # RGB to BGR for OpenCV\n",
    "\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    # Detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Information to show on the screen\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # For each detection from each output layer, get the confidence, class id, bounding box params\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and classes[class_id] == \"person\":  # Filter for 'person'\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-max suppression to eliminate redundant overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if len(indexes) == 0:\n",
    "        return None\n",
    "\n",
    "    # Find the closest bounding box to the joint coordinates\n",
    "    min_distance = float('inf')\n",
    "    best_box = None\n",
    "\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        box_center_x = x + w / 2\n",
    "        box_center_y = y + h / 2\n",
    "\n",
    "        # Calculate the distance between the box center and the joint coordinates\n",
    "        distance = np.sqrt((box_center_x - np.mean(joint_coordinates[:, 0]))**2 +\n",
    "                           (box_center_y - np.mean(joint_coordinates[:, 1]))**2)\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_box = (x, y, w, h)\n",
    "\n",
    "    if best_box is not None:\n",
    "        x, y, w, h = best_box\n",
    "        cropped_img = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Convert back to PIL image\n",
    "        cropped_img = Image.fromarray(cropped_img[:, :, ::-1])  # BGR to RGB for PIL\n",
    "        return cropped_img\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"E:\\dummy.csv\"\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Process and crop images\n",
    "process_and_crop_images(csv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO Detection with Cropping based on Joints coordinates\n",
    "# Error fixed\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Define the emotions to be considered\n",
    "valid_emotions = [\"Aversion\", \"Anger\", \"Peace\", \"Happiness\", \"Sadness\", \"Fear\", \"Surprise\"]\n",
    "\n",
    "def modify_image_folder_path(image_folder, valid_emotion):\n",
    "    # Replace \"ThirdPart\" with the valid emotion\n",
    "    modified_image_folder = image_folder.replace(\"ThirdPart\", valid_emotion)\n",
    "    return modified_image_folder\n",
    "\n",
    "def process_and_crop_images(csv_data):\n",
    "    for index, row in csv_data.iterrows():\n",
    "        image_folder = row['Video Path']\n",
    "        emotion_values = row['Emotion Value'].split(',')\n",
    "        start_frame = row['Start Frame']\n",
    "        end_frame = row['End Frame']\n",
    "        npy_path = row['Npy Path']\n",
    "        \n",
    "        # Process each emotion value separately\n",
    "        for emotion in emotion_values:\n",
    "            valid_emotion = emotion.strip()\n",
    "            if valid_emotion in valid_emotions:\n",
    "                # Modify the image folder path based on the valid emotion\n",
    "                modified_image_folder = modify_image_folder_path(image_folder, valid_emotion)\n",
    "                \n",
    "                # Check if the modified image folder exists\n",
    "                if not os.path.exists(modified_image_folder):\n",
    "                    print(f\"Error: Modified image folder '{modified_image_folder}' does not exist for emotion '{valid_emotion}'.\")\n",
    "                    continue\n",
    "                \n",
    "                # Debugging: Print the modified image folder path\n",
    "                print(f\"Processing images for emotion '{valid_emotion}' in folder '{modified_image_folder}'\")\n",
    "\n",
    "                # Load the .npy file for this specific folder\n",
    "                if not os.path.exists(npy_path):\n",
    "                    print(f\"Error: NPY file '{npy_path}' does not exist for emotion '{valid_emotion}'.\")\n",
    "                    continue\n",
    "                \n",
    "                npy_data = np.load(npy_path)\n",
    "                \n",
    "                # Calculate the number of frames to process\n",
    "                frame_count = end_frame - start_frame + 1  # Include both start and end frames\n",
    "                \n",
    "                # Ensure we don't exceed the available data in npy file\n",
    "                if frame_count > len(npy_data):\n",
    "                    print(f\"Error: The number of frames to process ({frame_count}) exceeds the available data in the npy file ({len(npy_data)}).\")\n",
    "                    continue\n",
    "                \n",
    "                # Process the frames within the specified range\n",
    "                for i in range(frame_count):\n",
    "                    frame_number = start_frame + i\n",
    "                    frame = npy_data[i]  # Get the i-th row from the npy file\n",
    "                    person_id = int(frame[1])\n",
    "                    joint_coordinates = frame[2:].reshape((18, 3))\n",
    "                    \n",
    "                    # Load the corresponding image\n",
    "                    image_path = os.path.join(modified_image_folder, f\"frame_{frame_number}.jpg\")\n",
    "                    \n",
    "                    # Debugging: Print the image path being checked\n",
    "                    print(f\"Looking for image at path '{image_path}'\")\n",
    "                    \n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Error: Image '{image_path}' does not exist for frame '{frame_number}' and emotion '{valid_emotion}'.\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        image = Image.open(image_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: Failed to open image '{image_path}' for frame '{frame_number}' and emotion '{valid_emotion}'. Exception: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Detect and crop the human closest to the joint coordinates\n",
    "                    cropped_image = detect_and_crop(image, joint_coordinates)\n",
    "                    \n",
    "                    if cropped_image is None:\n",
    "                        print(f\"Error: No cropped images generated for frame '{frame_number}' and emotion '{valid_emotion}'.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Save the cropped image\n",
    "                    cropped_image_path = image_path  # Save the cropped image to the original path\n",
    "                    try:\n",
    "                        cropped_image.save(cropped_image_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: Failed to save cropped image '{cropped_image_path}' for frame '{frame_number}', person '{person_id}' and emotion '{valid_emotion}'. Exception: {e}\")\n",
    "            else:\n",
    "                # Print an error message if no valid emotion is found\n",
    "                print(f\"Error: '{valid_emotion}' is not a valid emotion for row {index}. Skipping this emotion.\")\n",
    "\n",
    "# Updated detect_and_crop function with boundary checking\n",
    "def detect_and_crop(image, joint_coordinates):\n",
    "    # Convert PIL image to OpenCV format\n",
    "    img = np.array(image)\n",
    "    img = img[:, :, ::-1].copy()  # RGB to BGR for OpenCV\n",
    "\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    # Detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Information to show on the screen\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # For each detection from each output layer, get the confidence, class id, bounding box params\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and classes[class_id] == \"person\":  # Filter for 'person'\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-max suppression to eliminate redundant overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if len(indexes) == 0:\n",
    "        return None\n",
    "\n",
    "    # Find the closest bounding box to the joint coordinates\n",
    "    min_distance = float('inf')\n",
    "    best_box = None\n",
    "\n",
    "    for i in indexes.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        box_center_x = x + w / 2\n",
    "        box_center_y = y + h / 2\n",
    "\n",
    "        # Calculate the distance between the box center and the joint coordinates\n",
    "        distance = np.sqrt((box_center_x - np.mean(joint_coordinates[:, 0]))**2 +\n",
    "                           (box_center_y - np.mean(joint_coordinates[:, 1]))**2)\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_box = (x, y, w, h)\n",
    "\n",
    "    if best_box is not None:\n",
    "        x, y, w, h = best_box\n",
    "\n",
    "        # Clamp the coordinates to be within the image boundaries\n",
    "        x = max(0, x)\n",
    "        y = max(0, y)\n",
    "        w = min(w, width - x)\n",
    "        h = min(h, height - y)\n",
    "\n",
    "        cropped_img = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Convert back to PIL image\n",
    "        cropped_img = Image.fromarray(cropped_img[:, :, ::-1])  # BGR to RGB for PIL\n",
    "        return cropped_img\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(r\"E:\\YOLO\\yolov3.weights\", r\"E:\\YOLO\\yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "# Adjust for different return types of getUnconnectedOutLayers\n",
    "try:\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "except IndexError:\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO labels\n",
    "with open(r\"E:\\YOLO\\coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"E:\\annotation_train_2.0.csv\"\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Process and crop images\n",
    "process_and_crop_images(csv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Path to the main folder containing emotion folders\n",
    "main_folder_path = r\"E:\\train_gen_joints\"\n",
    "\n",
    "# List of emotion folders\n",
    "emotions = ['Anger', 'Happiness', 'Sadness', 'Aversion', 'Peace', 'Surprise', 'Fear']\n",
    "\n",
    "for emotion in emotions:\n",
    "    emotion_folder_path = os.path.join(main_folder_path, emotion)\n",
    "    \n",
    "    for root, dirs, files in os.walk(emotion_folder_path, topdown=False):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            new_path = os.path.join(emotion_folder_path, file)\n",
    "            \n",
    "            # If the file already exists, add a unique suffix\n",
    "            if os.path.exists(new_path):\n",
    "                base, ext = os.path.splitext(file)\n",
    "                counter = 1\n",
    "                new_name = f\"{base}_{counter}{ext}\"\n",
    "                new_path = os.path.join(emotion_folder_path, new_name)\n",
    "                while os.path.exists(new_path):\n",
    "                    counter += 1\n",
    "                    new_name = f\"{base}_{counter}{ext}\"\n",
    "                    new_path = os.path.join(emotion_folder_path, new_name)\n",
    "            \n",
    "            # Move the file to the emotion folder\n",
    "            shutil.move(file_path, new_path)\n",
    "        \n",
    "        # Remove empty directories\n",
    "        for dir in dirs:\n",
    "            dir_path = os.path.join(root, dir)\n",
    "            if not os.listdir(dir_path):\n",
    "                os.rmdir(dir_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
